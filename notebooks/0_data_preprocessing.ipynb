{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas matplotlib python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43487/2591596369.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DOT_ENV = \"../src/.env\"\n",
    "\n",
    "# load environment variables from the .env file\n",
    "load_dotenv(PATH_TO_DOT_ENV)\n",
    "\n",
    "# extract environment variables\n",
    "APTOS_2019_ZIP_PATH = \"..\" + os.environ.get(\"APTOS_2019_ZIP_PATH\")\n",
    "# APTOS_2019_TEST_CSV = os.environ.get(\"APTOS_2019_TEST_CSV\")\n",
    "APTOS_2019_TRAIN_CSV = os.environ.get(\"APTOS_2019_TRAIN_CSV\")\n",
    "APTOS_2019_ID_COLUMN = os.environ.get(\"APTOS_2019_ID_COLUMN\")\n",
    "APTOS_2019_LABLE_COLUMN = os.environ.get(\"APTOS_2019_LABLE_COLUMN\")\n",
    "APTOS_2019_FILE_EXTENSION = os.environ.get(\"APTOS_2019_FILE_EXTENSION\")\n",
    "\n",
    "PROCESSED_DATASET = \"..\" + os.environ.get(\"PROCESSED_DATASET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset Meta Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(APTOS_2019_ZIP_PATH, 'r') as raw_dataset_zip:\n",
    "    # # get a list of all file names in the zip archive\n",
    "    # compressed_file_names = raw_dataset_zip.namelist()\n",
    "    \n",
    "    # load train file\n",
    "    with raw_dataset_zip.open(APTOS_2019_TRAIN_CSV) as raw_train_csv:\n",
    "        train_df = pd.read_csv(raw_train_csv, index_col=APTOS_2019_ID_COLUMN)\n",
    "\n",
    "    # # load test file\n",
    "    # with raw_dataset_zip.open(APTOS_2019_TEST_CSV) as raw_test_csv:\n",
    "    #     test_df = pd.read_csv(raw_test_csv, index_col=APTOS_2019_ID_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indentify Lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique lables: [2 4 1 0 3]\n"
     ]
    }
   ],
   "source": [
    "lables = train_df[APTOS_2019_LABLE_COLUMN].unique()\n",
    "print(f\"Unique lables: {lables}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratisfied Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide Images by Lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the train set by lables \n",
    "lables_index = {\n",
    "    lable: train_df[train_df[APTOS_2019_LABLE_COLUMN] == lable].index.to_list() for lable in lables\n",
    "}\n",
    "\n",
    "# validated the lable division\n",
    "split_lable_count = sum([len(values) for values in lables_index.values()])\n",
    "assert len(train_df) == split_lable_count, f\"Labes index were not split properly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Lable into Train, Test, and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished splitting the lable: '2' into train (799), test (101), and validation (99)\n",
      "Finished splitting the lable: '4' into train (236), test (30), and validation (29)\n",
      "Finished splitting the lable: '1' into train (296), test (37), and validation (37)\n",
      "Finished splitting the lable: '0' into train (1444), test (181), and validation (180)\n",
      "Finished splitting the lable: '3' into train (154), test (20), and validation (19)\n"
     ]
    }
   ],
   "source": [
    "porcessed_dataset_index = {\n",
    "    \"train\": {},\n",
    "    \"test\": {},\n",
    "    \"val\": {}\n",
    "}\n",
    "\n",
    "for lable in lables:\n",
    "    # get the size of dataset for the respective lable\n",
    "    len_label = len(lables_index[lable])\n",
    "    train_size = int(len_label * 0.8)\n",
    "    val_size = int(len_label * 0.1)\n",
    "    test_size = len_label - train_size - val_size\n",
    "\n",
    "    # shuffle the datset to randomly sampple\n",
    "    random.shuffle(lables_index[lable])\n",
    "\n",
    "    # perform a stratisfied split of the lable into train, test, and validation\n",
    "    porcessed_dataset_index[\"train\"][lable] = lables_index[lable][ :train_size]\n",
    "    porcessed_dataset_index[\"test\"][lable] = lables_index[lable][train_size : train_size + test_size]\n",
    "    porcessed_dataset_index[\"val\"][lable] = lables_index[lable][train_size + test_size : ]\n",
    "\n",
    "    # validate test for the split \n",
    "    assert len(lables_index[lable]) == len(porcessed_dataset_index[\"train\"][lable]) + len(porcessed_dataset_index[\"test\"][lable]) + len(porcessed_dataset_index[\"val\"][lable]), f\"lable split count does not add up\"\n",
    "\n",
    "    print(f\"Finished splitting the lable: '{lable}' into train ({len(porcessed_dataset_index['train'][lable])}), test ({len(porcessed_dataset_index['test'][lable])}), and validation ({len(porcessed_dataset_index['val'][lable])})\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data from zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_processed_dir = lambda set_type, lable: f\"{PROCESSED_DATASET}{set_type}/{lable}\"\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "for lable in lables:\n",
    "    os.makedirs(gen_processed_dir(\"train\", lable), exist_ok=True)\n",
    "    os.makedirs(gen_processed_dir(\"test\", lable), exist_ok=True)\n",
    "    os.makedirs(gen_processed_dir(\"val\", lable), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started set: train\n",
      "Extracted lable: 2\n",
      "Extracted lable: 4\n",
      "Extracted lable: 1\n",
      "Extracted lable: 0\n",
      "Extracted lable: 3\n",
      "Finished set: train\n",
      "Started set: test\n",
      "Extracted lable: 2\n",
      "Extracted lable: 4\n",
      "Extracted lable: 1\n",
      "Extracted lable: 0\n",
      "Extracted lable: 3\n",
      "Finished set: test\n",
      "Started set: val\n",
      "Extracted lable: 2\n",
      "Extracted lable: 4\n",
      "Extracted lable: 1\n",
      "Extracted lable: 0\n",
      "Extracted lable: 3\n",
      "Finished set: val\n"
     ]
    }
   ],
   "source": [
    "SET_TYPES = [\"train\", \"test\", \"val\"]\n",
    "\n",
    "with zipfile.ZipFile(APTOS_2019_ZIP_PATH, 'r') as raw_dataset_zip:\n",
    "    for set_type in SET_TYPES:\n",
    "        print(f\"Started set: {set_type}\")\n",
    "        for lable in lables:\n",
    "            for image_name in porcessed_dataset_index[set_type][lable]:\n",
    "                zip_image_path = f\"train_images/{image_name}.png\"\n",
    "                processed_image_path = f\"{gen_processed_dir(set_type, lable)}\"\n",
    "                raw_dataset_zip.extract(zip_image_path, processed_image_path)\n",
    "                os.rename(f\"{processed_image_path}/{zip_image_path}\", f\"{processed_image_path}/{image_name}.png\")\n",
    "            os.removedirs(f\"{processed_image_path}/train_images/\")\n",
    "            print(f\"Extracted lable: {lable}\")\n",
    "        print(f\"Finished set: {set_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test preprocessing\n",
    "processed_len = 0\n",
    "for set_type in SET_TYPES:\n",
    "    for lable in lables:\n",
    "        labeled_set_dir = f\"{gen_processed_dir(set_type, lable)}\"\n",
    "        processed_len += len(os.listdir(labeled_set_dir))\n",
    "    \n",
    "assert processed_len == len(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
